{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/slremy/netsapi\n",
      "  Cloning https://github.com/slremy/netsapi to /private/var/folders/qj/6zzv4hhx5fzbkm01x5qr69800000gn/T/pip-req-build-8827peqc\n",
      "  Running command git clone -q https://github.com/slremy/netsapi /private/var/folders/qj/6zzv4hhx5fzbkm01x5qr69800000gn/T/pip-req-build-8827peqc\n",
      "Building wheels for collected packages: netsapi\n",
      "  Building wheel for netsapi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/qj/6zzv4hhx5fzbkm01x5qr69800000gn/T/pip-ephem-wheel-cache-dxqfqhs6/wheels/9e/73/c9/86a9cc2460e11b3ce5b0a5ebd2d9d332a68afe0941659967fa\n",
      "Successfully built netsapi\n",
      "Installing collected packages: netsapi\n",
      "  Found existing installation: netsapi 1.1\n",
      "    Uninstalling netsapi-1.1:\n",
      "      Successfully uninstalled netsapi-1.1\n",
      "Successfully installed netsapi-1.1\n"
     ]
    }
   ],
   "source": [
    "from sys import exit, exc_info, argv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n",
    "\n",
    "from netsapi.challenge import *\n",
    "from netsapi.visualisation import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x13edae2f0>, {})\n",
      "random_action 3\n",
      "env_action [0.0, 0.5]\n",
      "105  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "104  Evaluations Remaining\n",
      "random_action 3\n",
      "env_action [0.0, 0.5]\n",
      "103  Evaluations Remaining\n",
      "random_action 4\n",
      "env_action [0.5, 0.5]\n",
      "102  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "101  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "100  Evaluations Remaining\n",
      "random_action 2\n",
      "env_action [1.0, 0.0]\n",
      "99  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "98  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "97  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "96  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "95  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "94  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "93  Evaluations Remaining\n",
      "random_action 0\n",
      "env_action [0.0, 0.0]\n",
      "92  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "91  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "90  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "89  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "88  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "87  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "86  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "85  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "84  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "83  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "82  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "81  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "80  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "79  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "78  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "77  Evaluations Remaining\n",
      "env_action [0.0, 0.0]\n",
      "76  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "75  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "74  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "73  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "72  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "71  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "70  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "69  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "68  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "67  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "66  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "65  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "64  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "63  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "62  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "61  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "60  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "59  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "58  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "57  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "56  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "55  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "54  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "53  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "52  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "51  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "50  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "49  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "48  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "47  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "46  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "45  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "44  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "43  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "42  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "41  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "40  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "39  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "38  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "37  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "36  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "35  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "34  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "33  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "32  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "31  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "30  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "29  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "28  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "27  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "26  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "25  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "24  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "23  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "22  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "21  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "20  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "19  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "18  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "17  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "16  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "15  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "14  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "13  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "12  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "11  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "10  Evaluations Remaining\n",
      "env_action [1.0, 0.0]\n",
      "9  Evaluations Remaining\n",
      "env_action [0.0, 0.5]\n",
      "8  Evaluations Remaining\n",
      "env_action [0.5, 0.5]\n",
      "7  Evaluations Remaining\n",
      "env_action [0.5, 0.0]\n",
      "6  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [0.0, 0.0], 2: [0.0, 0.5], 3: [1.0, 0.0], 4: [0.0, 0.5], 5: [0.5, 0.5]} 146.84843235289247\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "env = ChallengeSeqDecEnvironment()\n",
    "\n",
    "Q = defaultdict(lambda : 0.) # Q-function\n",
    "n = defaultdict(lambda : 1.) # number of visits\n",
    "\n",
    "def actionSpace(resolution):\n",
    "    x,y = np.meshgrid(np.arange(0,1+resolution,resolution), np.arange(0,1+resolution,resolution))\n",
    "    xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "    return xy.round(2).tolist()\n",
    "\n",
    "#HyperParameters\n",
    "epsilon = 0.9\n",
    "gamma = 0.8\n",
    "action_resolution = 0.5\n",
    "episode_number = 20 #for submission this is fixed as 20\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "#Set-up\n",
    "actions = actionSpace(action_resolution)\n",
    "actionspace = range(len(actions)-1)\n",
    "greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "count = 0\n",
    "\n",
    "policies = []\n",
    "rewards = []\n",
    "#Training of Q Table\n",
    "\n",
    "print(Q)\n",
    "\n",
    "for _ in range(episode_number):\n",
    "    env.reset()\n",
    "    nextstate = env.state\n",
    "    while True:\n",
    "        state = nextstate\n",
    "        # Epsilon-Greedy\n",
    "        if epsilon > random.random() :\n",
    "            action = random.choice(actionspace)\n",
    "            print('random_action',action)\n",
    "        else :\n",
    "            action = greedy_action(state)\n",
    "\n",
    "        env_action = actions[action] #convert to ITN/IRS\n",
    "        print('env_action', env_action)\n",
    "        nextstate, reward, done, _ = env.evaluateAction(env_action)\n",
    "    \n",
    "        epsilon = epsilon * 0.8\n",
    "        # Q-learning\n",
    "        \"\"\"\n",
    "        if done :\n",
    "            Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "            break\n",
    "        else :\n",
    "            Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "        \"\"\"\n",
    "        if done :\n",
    "            Q[(state,action)] = Q[(state,action)] + learning_rate * ( reward - Q[(state,action)] )\n",
    "            break\n",
    "        else :\n",
    "            Q[(state,action)] = Q[(state,action)] + learning_rate * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "        \n",
    "        learning_rate = learning_rate * 0.999\n",
    "        \n",
    "#Greedy Policy Learnt from Q Table\n",
    "best_policy = {state: list(actions[greedy_action(state-1)]) for state in range(1,6)}\n",
    "best_reward = env.evaluatePolicy(best_policy)\n",
    "print(best_policy, best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {(1, 3): 117.45582055305213,\n",
       "             (2, 0): 0.1670761712022857,\n",
       "             (2, 1): 0.0,\n",
       "             (2, 2): 111.18104183801076,\n",
       "             (2, 3): 0.0,\n",
       "             (2, 4): 0.0,\n",
       "             (2, 5): 0.0,\n",
       "             (2, 6): 0.0,\n",
       "             (2, 7): 0.0,\n",
       "             (3, 0): 0.0,\n",
       "             (3, 1): 0.0,\n",
       "             (3, 2): 0.0,\n",
       "             (3, 3): 31.93006591399085,\n",
       "             (3, 4): 0.0,\n",
       "             (3, 5): 0.0,\n",
       "             (3, 6): 0.0,\n",
       "             (3, 7): 0.0,\n",
       "             (4, 0): 0.3028866938562811,\n",
       "             (4, 1): 0.0,\n",
       "             (4, 2): 0.0,\n",
       "             (4, 3): 0.0,\n",
       "             (4, 4): 7.338069129121742,\n",
       "             (4, 5): 0.0,\n",
       "             (4, 6): 0.0,\n",
       "             (4, 7): 0.0,\n",
       "             (5, 0): -0.18128846236414115,\n",
       "             (5, 1): 1.487735438507334,\n",
       "             (5, 2): 0.0,\n",
       "             (5, 3): 0.0,\n",
       "             (5, 4): 0.0,\n",
       "             (5, 5): 0.0,\n",
       "             (5, 6): 0.0,\n",
       "             (5, 7): 0.0,\n",
       "             (1, 0): 0.0,\n",
       "             (1, 1): 0.0,\n",
       "             (1, 2): 0.0,\n",
       "             (1, 4): 0.0,\n",
       "             (1, 5): 0.0,\n",
       "             (1, 6): 0.0,\n",
       "             (1, 7): 0.0,\n",
       "             (0, 0): 0.0,\n",
       "             (0, 1): 0.0,\n",
       "             (0, 2): 0.0,\n",
       "             (0, 3): 0.0,\n",
       "             (0, 4): 0.0,\n",
       "             (0, 5): 0.0,\n",
       "             (0, 6): 0.0,\n",
       "             (0, 7): 0.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* action space [0, 0.5, 1] : \n",
    "    * a = 1\n",
    "        * epsilon = 0.9 => 96.58\n",
    "        * epsilon = 0.9 * (0.99)^loop => 76.43\n",
    "        * epsilon = 0.9 * (0.9)^loop => 177.65\n",
    "        * epsilon = 0.9 * (0.8)^loop => 145.29\n",
    "    * a = 0.7\n",
    "        * epsilon = 0.9 * (0.8)^loop => 49.27\n",
    "    * a = 0.8\n",
    "        * epsilon = 0.9 * (0.8)^loop => 96.61\n",
    "    * a = 0.9\n",
    "        * epsilon = 0.9 * (0.8)^loop => 160.8667968042252\n",
    "    * a = 1 - 0.01*loop\n",
    "        * epsilon = 0.9 * (0.8)^loop => 101.106\n",
    "    * a = a*0.99, a = 1\n",
    "        * epsilon = 0.9 * (0.8)^loop => 146.84\n",
    "    \n",
    "* action space [0, 0.2, 0.4, ..., 1.0]: epsilon = 0.9 * (0.99)^loop => 125.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.env = environment\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.8\n",
    "        self.action_resolution = 0.2\n",
    "        self.Q = defaultdict(lambda : 0.) # Q-function\n",
    "        self.n = defaultdict(lambda : 1.) # number of visits\n",
    "        self.actions = actionSpace(self.action_resolution)\n",
    "        self.actionspace = range(len(self.actions)-1)\n",
    "        \n",
    "    \n",
    "    def actionSpace(self):\n",
    "        x,y = np.meshgrid(np.arange(0,1+self.action_resolution,self.action_resolution), np.arange(0,1+self.action_resolution,self.action_resolution))\n",
    "        xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "        return xy.round(2).tolist()\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        Q = self.Q\n",
    "        n = self.n\n",
    "        actions = self.actions\n",
    "        actionspace = self.actionspace\n",
    "\n",
    "        greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "        max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "\n",
    "        \n",
    "        for _ in range(20): #Do not change\n",
    "            \n",
    "            self.env.reset()\n",
    "            nextstate = self.env.state\n",
    "            \n",
    "            while True:\n",
    "                state = nextstate\n",
    "\n",
    "                # Epsilon-Greedy Action Selection\n",
    "                if epsilon > random.random() :\n",
    "                    action = random.choice(actionspace)\n",
    "                else :\n",
    "                    action = greedy_action(state)\n",
    "\n",
    "                env_action = actions[action]#convert to ITN/IRS\n",
    "                print('env_action', env_action)\n",
    "                nextstate, reward, done, _ = self.env.evaluateAction(env_action)\n",
    "\n",
    "                # Q-learning\n",
    "                if done :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "                    break\n",
    "                else :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        \n",
    "        Q_trained = self.train()\n",
    "        greedy_eval = lambda s : max(actionspace, key=lambda a : Q_trained[(s,a)])\n",
    "        \n",
    "        best_policy = {state: list(actions[greedy_eval(state-1)]) for state in range(1,6)}\n",
    "        best_reward = self.env.evaluatePolicy(best_policy)\n",
    "        \n",
    "        print(best_policy, best_reward)\n",
    "        \n",
    "        return best_policy, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, Q_Agent, \"Q_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netsapi.challenge import *\n",
    "\n",
    "class CustomAgent:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        candidates = []\n",
    "        try:\n",
    "            # Agents should make use of 20 episodes in each training run, if making sequential decisions\n",
    "            for i in range(20):\n",
    "                self.environment.reset()\n",
    "                policy = {}\n",
    "                for j in range(5): #episode length\n",
    "                    x = 0\n",
    "                    y = 0\n",
    "                    ran = random.random()\n",
    "                    if ran > 0.7:\n",
    "                        x = 1\n",
    "                    elif ran < 0.7 and ran > 0.3:\n",
    "                        x = 0.5\n",
    "                        y = 0.5\n",
    "                    else:\n",
    "                        y = 1\n",
    "                    policy[str(j+1)]=[x, y]\n",
    "                candidates.append(policy)\n",
    "                \n",
    "            rewards = self.environment.evaluatePolicy(candidates)\n",
    "            best_policy = candidates[np.argmax(rewards)]\n",
    "            best_reward = rewards[np.argmax(rewards)]\n",
    "        \n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            print(exc_info())\n",
    "            \n",
    "        return best_policy, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "399.60788879132554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<netsapi.challenge.EvaluateChallengeSubmission at 0x13e775710>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, CustomAgent, \"example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* getting [0.5, 0.5]\n",
    "    * 20%: all with [0.5 0.5] are worst 336, 357\n",
    "    * 40%: 336, 412, 423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
